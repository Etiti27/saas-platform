name: Test, Build & Deploy React Vite to EKS

on:
  push:
    branches: [cicd]
  workflow_dispatch:

concurrency:
  group: cicd-${{ github.ref }}
  cancel-in-progress: true

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: SaaS-cluster-v3
  IMAGE_NAME: saas-frontend
  IMAGE_VERSION: v1.0.0
  DOCKERHUB_USERNAME: obinna27
  NODE_COUNT: 2
  K8S_VERSION: "1.31"
  NODE_TYPE: t3.small
  HELM_RELEASE_NAME: saas-helm
  CHART_NAME: bitnami/nginx
  CHART_VERSION: 15.5.2
  NAMESPACE: default

jobs:
  Test:
    name: Run frontend tests
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: frontend
    steps:
      - name: 🔄 Checkout Source Code
        uses: actions/checkout@v4

      - name: 🟢 Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm
          cache-dependency-path: frontend/package-lock.json

      - name: 📦 Install deps
        run: npm ci

      - name: ✅ Run tests
        run: npm test --if-present

  Build:
    name: Build & push Docker image
    runs-on: ubuntu-latest
    needs: Test
    steps:
      - name: 🔄 Checkout Source Code
        uses: actions/checkout@v4

      - name: ⚙️ Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: 🔐 Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ env.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: 🏗️ Build and Push Docker Image
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          platforms: linux/amd64,linux/arm64
          push: true
          tags: |
            ${{ env.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:${{ env.IMAGE_VERSION }}
            ${{ env.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:${{ github.sha }}

  deploy:
    name: Deploy Frontend to EKS
    runs-on: ubuntu-latest
    needs: Build
    steps:
      - name: 🔄 Checkout Source Code
        uses: actions/checkout@v4

      - name: 🔐 Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: 📦 Install kubectl
        run: |
          curl -sL https://storage.googleapis.com/kubernetes-release/release/$(curl -sL https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl -o kubectl
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: 📥 Install eksctl
        run: |
          curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \
          | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version

      # 🔎 Detect orphaned eksctl stacks to avoid AlreadyExistsException
      - name: 🧪 Preflight: detect orphaned eksctl stacks
        id: preflight
        run: |
          set -e
          STACK_CLUSTER="eksctl-${{ env.CLUSTER_NAME }}-cluster"
          STACK_NG="eksctl-${{ env.CLUSTER_NAME }}-nodegroup-standard-workers"

          cluster_exists=false
          if aws eks describe-cluster --region "${{ env.AWS_REGION }}" --name "${{ env.CLUSTER_NAME }}" >/dev/null 2>&1; then
            cluster_exists=true
          fi

          stack_exists=false
          if aws cloudformation describe-stacks --region "${{ env.AWS_REGION }}" --stack-name "$STACK_CLUSTER" >/dev/null 2>&1; then
            stack_exists=true
          fi

          echo "cluster_exists=$cluster_exists" >> $GITHUB_OUTPUT
          echo "stack_exists=$stack_exists" >> $GITHUB_OUTPUT

      - name: 🧹 Cleanup orphaned CloudFormation (if any)
        if: steps.preflight.outputs.cluster_exists == 'false' && steps.preflight.outputs.stack_exists == 'true'
        run: |
          set -e
          STACK_CLUSTER="eksctl-${{ env.CLUSTER_NAME }}-cluster"
          STACK_NG="eksctl-${{ env.CLUSTER_NAME }}-nodegroup-standard-workers"

          eksctl delete cluster --region "${{ env.AWS_REGION }}" --name "${{ env.CLUSTER_NAME }}" || true

          aws cloudformation delete-stack --region "${{ env.AWS_REGION }}" --stack-name "$STACK_NG" || true
          aws cloudformation delete-stack --region "${{ env.AWS_REGION }}" --stack-name "$STACK_CLUSTER" || true

          aws cloudformation wait stack-delete-complete --region "${{ env.AWS_REGION }}" --stack-name "$STACK_NG" || true
          aws cloudformation wait stack-delete-complete --region "${{ env.AWS_REGION }}" --stack-name "$STACK_CLUSTER" || true

      - name: 🧪 Check if EKS cluster exists
        id: cluster-check
        run: |
          if aws eks describe-cluster --region "${{ env.AWS_REGION }}" --name "${{ env.CLUSTER_NAME }}" >/dev/null 2>&1; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: 🚀 Create EKS cluster (if missing)
        if: steps.cluster-check.outputs.exists == 'false'
        run: |
          eksctl create cluster \
            --name "${{ env.CLUSTER_NAME }}" \
            --region "${{ env.AWS_REGION }}" \
            --version "${{ env.K8S_VERSION }}" \
            --nodegroup-name "standard-workers" \
            --node-type "${{ env.NODE_TYPE }}" \
            --nodes "${{ env.NODE_COUNT }}" \
            --nodes-min 1 \
            --nodes-max 4 \
            --managed

      - name: 🔗 Update kubeconfig
        run: aws eks --region "${{ env.AWS_REGION }}" update-kubeconfig --name "${{ env.CLUSTER_NAME }}"

      - name: 📦 Install Helm
        run: curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash

      - name: ➕ Add Helm Repositories
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add jetstack https://charts.jetstack.io
          helm repo update

      - name: 🚀 Install NGINX Ingress Controller
        run: |
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --set controller.publishService.enabled=true
          kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx

      - name: 🔐 Install cert-manager
        run: |
          helm upgrade --install cert-manager jetstack/cert-manager \
            --namespace cert-manager \
            --create-namespace \
            --version v1.15.0 \
            --set installCRDs=true
          kubectl rollout status deployment/cert-manager -n cert-manager
          kubectl rollout status deployment/cert-manager-webhook -n cert-manager
          kubectl rollout status deployment/cert-manager-cainjector -n cert-manager

      - name: 📜 Apply ClusterIssuer and Frontend manifests
        run: |
          kubectl apply -f ./k8s/clusterissuer.yaml
          kubectl apply -f ./k8s/frontend.yaml
